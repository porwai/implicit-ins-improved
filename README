LESS is More: Data-Efficient Implicit Instruction Tuning

This repo contains code and experiments from our ACL 2025 paper, "LESS is More: Data-Efficient Implicit Instruction Tuning of Large Language Models." We explore how to make instruction tuning more efficient by selecting better training examples. We fine-tune LLaMA-2-7B on 1k-example subsets of OpenMathInstruct-1 using three selection methods: LESS (influence-based), Apricot-FL, and MaxDist. Our results show LESS outperforms other strategies on AlpacaEval 2.

ðŸ“§ Contact: km7047@princeton.edu | xh3195@princeton.edu | pw5115@princeton.edu
