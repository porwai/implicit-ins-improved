
---

## ğŸ“š Dataset Preparation

This project uses four datasets to support instruction tuning and evaluation:

- ğŸ§  **LIMA**: General instruction-response pairs
- ğŸ“ **GSM8K**: Grade school math questions
- âœï¸ **Poetry**: Poem generation from titles
- ğŸ“Š **OpenMathInstruct-1**: Large-scale math instruction dataset (1.8M examples, we use a 10K subset)

---

### ğŸ›  How to Load All Datasets

We've provided a single script to download and save all datasets in a consistent instruction-response format.

#### âœ… Step-by-step:

1. **Activate your environment**:
   ```bash
   source .venv/bin/activate
   ```

2. **Run the unified loader**:
   ```bash
   python scripts/load_all.py
   ```

3. **What it does**:
   - Downloads datasets using Hugging Face `datasets` library
   - Converts them into a standard format:
     ```json
     {
       "instruction": "...",
       "response": "..."
     }
     ```
   - Saves each one to the `data/` folder:
     - `data/lima.json`
     - `data/gsm8k.json`
     - `data/poetry.json`
     - `data/openmath_full.json` (10,000 samples)

---

### ğŸ“‚ Output Format

Each JSON file contains a list of instruction-response pairs:
```json
[
  {
    "instruction": "What is 7 x 6?",
    "response": "The answer is 42."
  },
  ...
]
```

---

### âš™ï¸ Modify the script

If you want to increase the number of OpenMath examples (default: 10K), edit this line in `scripts/load_all.py`:
```python
save_dataset("openmath_full", load_openmath(limit=10000))
```

---

